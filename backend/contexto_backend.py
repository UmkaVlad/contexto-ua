"""
Contexto UA ‚Äî –±–µ–∫–µ–Ω–¥ –¥–ª—è –≥—Ä–∏.
–ü—Ä–∞—Ü—é—î –∑ —Ñ–∞–π–ª–∞–º–∏: clean_words.txt —Ç–∞ embeddings_ua.pt
"""
import os
import random
from pathlib import Path
from datetime import date
import hashlib

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel

try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

app = FastAPI(title="Contexto UA API")

# –î–æ–∑–≤–æ–ª—è—î–º–æ –∑–∞–ø–∏—Ç–∏ –∑ –±—Ä–∞—É–∑–µ—Ä–∞ (CORS)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- –®–õ–Ø–•–ò –î–û –§–ê–ô–õ–Ü–í ---
BASE = Path(__file__).resolve().parent.parent
# –¢–µ–ø–µ—Ä –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –Ω–æ–≤—ñ —Ñ–∞–π–ª–∏, —è–∫—ñ –º–∏ —Å—Ç–≤–æ—Ä–∏–ª–∏
WORDS_PATH = BASE / "clean_words.txt"
EMBEDDINGS_PATH = BASE / "embeddings_ua.pt"

# –ì–ª–æ–±–∞–ª—å–Ω—ñ –∑–º—ñ–Ω–Ω—ñ
words_list: list[str] = []
word_to_index: dict[str, int] = {}
embeddings_tensor = None


def load_data():
    """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î —Å–ª–æ–≤–∞ —Ç–∞ –≤–µ–∫—Ç–æ—Ä–∏ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç—ñ —Å–µ—Ä–≤–µ—Ä–∞."""
    global words_list, word_to_index, embeddings_tensor

    # 1. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Å–ª—ñ–≤
    if not WORDS_PATH.exists():
        print(f"‚ùå –ü–û–ú–ò–õ–ö–ê: –ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª {WORDS_PATH}")
        return
    
    with open(WORDS_PATH, "r", encoding="utf-8") as f:
        # –ü—Ä–æ—Å—Ç–æ —á–∏—Ç–∞—î–º–æ —Ä—è–¥–∫–∏, –±–æ —Ñ–∞–π–ª –≤–∂–µ —á–∏—Å—Ç–∏–π
        words_list = [line.strip() for line in f if line.strip()]
    
    word_to_index = {w: i for i, w in enumerate(words_list)}
    print(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ —Å–ª—ñ–≤: {len(words_list)}")

    # 2. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –≤–µ–∫—Ç–æ—Ä—ñ–≤
    if TORCH_AVAILABLE and EMBEDDINGS_PATH.exists():
        print("‚è≥ –ó–∞–≤–∞–Ω—Ç–∞–∂—É—é –≤–µ–∫—Ç–æ—Ä–∏ (—Ü–µ –º–æ–∂–µ –∑–∞–π–Ω—è—Ç–∏ –∫—ñ–ª—å–∫–∞ —Å–µ–∫—É–Ω–¥)...")
        try:
            embeddings_tensor = torch.load(EMBEDDINGS_PATH, map_location="cpu")
            # –Ø–∫—â–æ –∑–∞–≤–∞–Ω—Ç–∞–∂–∏–≤—Å—è —Å–ø–∏—Å–æ–∫ —Ç–µ–Ω–∑–æ—Ä—ñ–≤, –ø–µ—Ä–µ—Ç–≤–æ—Ä—é—î–º–æ –≤ –æ–¥–∏–Ω –≤–µ–ª–∏–∫–∏–π —Ç–µ–Ω–∑–æ—Ä
            if isinstance(embeddings_tensor, list):
                 embeddings_tensor = torch.stack(embeddings_tensor)
            
            # –ü–µ—Ä–µ–∫–æ–Ω–∞—î–º–æ—Å—è, —â–æ —Ü–µ float32
            if not isinstance(embeddings_tensor, torch.Tensor):
                 embeddings_tensor = torch.tensor(embeddings_tensor, dtype=torch.float32)

            print(f"‚úÖ –í–µ–∫—Ç–æ—Ä–∏ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ! –†–æ–∑–º—ñ—Ä: {embeddings_tensor.shape}")
        except Exception as e:
            print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –≤–µ–∫—Ç–æ—Ä—ñ–≤: {e}")
            embeddings_tensor = None
    else:
        print(f"‚ö†Ô∏è –£–≤–∞–≥–∞: —Ñ–∞–π–ª –≤–µ–∫—Ç–æ—Ä—ñ–≤ {EMBEDDINGS_PATH} –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –∞–±–æ Torch –Ω–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ.")


@app.on_event("startup")
def startup():
    load_data()


def get_secret_index(level: int, seed_date: str) -> int:
    """–í–∏–±–∏—Ä–∞—î —Å–µ–∫—Ä–µ—Ç–Ω–µ —Å–ª–æ–≤–æ –Ω–∞ –æ—Å–Ω–æ–≤—ñ –¥–∞—Ç–∏ (—â–æ–± —É –≤—Å—ñ—Ö –±—É–ª–æ –æ–¥–Ω–∞–∫–æ–≤–µ)."""
    # –°—ñ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó (–º–æ–∂–Ω–∞ –∑–º—ñ–Ω–∏—Ç–∏)
    unique_str = f"contexto_ua_game_{seed_date}_{level}"
    h = hashlib.sha256(unique_str.encode()).hexdigest()
    # –ü–µ—Ä–µ—Ç–≤–æ—Ä—é—î–º–æ —Ö–µ—à –Ω–∞ —á–∏—Å–ª–æ —ñ –±–µ—Ä–µ–º–æ –ø–æ –º–æ–¥—É–ª—é –∫—ñ–ª—å–∫–æ—Å—Ç—ñ —Å–ª—ñ–≤
    idx = int(h[:10], 16) % len(words_list)
    return idx


def get_position(secret_idx: int, guess_idx: int) -> int:
    """–†–∞—Ö—É—î –ø–æ–∑–∏—Ü—ñ—é —Å–ª–æ–≤–∞ (1 = –ø–µ—Ä–µ–º–æ–≥–∞)."""
    if embeddings_tensor is None:
        return 9999 # –ó–∞–≥–ª—É—à–∫–∞, —è–∫—â–æ –≤–µ–∫—Ç–æ—Ä–∏ –Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–∏–ª–∏—Å—å

    # –í–µ–∫—Ç–æ—Ä —Å–µ–∫—Ä–µ—Ç–Ω–æ–≥–æ —Å–ª–æ–≤–∞
    target_vec = embeddings_tensor[secret_idx].unsqueeze(0)
    
    # –†–∞—Ö—É—î–º–æ —Å—Ö–æ–∂—ñ—Å—Ç—å –∑ —É—Å—ñ–º–∞ —Å–ª–æ–≤–∞–º–∏ –æ–¥—Ä–∞–∑—É (Cosine Similarity)
    # –§–æ—Ä–º—É–ª–∞: (A . B) / (|A| * |B|)
    sim_all = torch.nn.functional.cosine_similarity(embeddings_tensor, target_vec)
    
    # –°—Ö–æ–∂—ñ—Å—Ç—å –Ω–∞—à–æ–≥–æ —Å–ª–æ–≤–∞
    guess_sim = sim_all[guess_idx].item()
    
    # –ü–æ–∑–∏—Ü—ñ—è = (–∫—ñ–ª—å–∫—ñ—Å—Ç—å —Å–ª—ñ–≤, —É —è–∫–∏—Ö —Å—Ö–æ–∂—ñ—Å—Ç—å –ë–Ü–õ–¨–®–ê –Ω—ñ–∂ —É –Ω–∞—à–æ–≥–æ) + 1
    position = (sim_all > guess_sim).sum().item() + 1
    
    return position


# --- API ---

class GuessRequest(BaseModel):
    level: int = 1
    seed_date: str = "" # YYYY-MM-DD
    word: str

class GuessResponse(BaseModel):
    position: int
    normalized_word: str
    found: bool
    error: str = ""

@app.post("/api/guess", response_model=GuessResponse)
def api_guess(req: GuessRequest):
    if not req.seed_date:
        req.seed_date = date.today().isoformat()

    guess_word = req.word.strip().lower()

    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞: —á–∏ —î —Å–ª–æ–≤–æ –≤ —Å–ª–æ–≤–Ω–∏–∫—É
    if guess_word not in word_to_index:
        return GuessResponse(position=0, normalized_word=guess_word, found=False, error="–°–ª–æ–≤–æ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")

    secret_idx = get_secret_index(req.level, req.seed_date)
    guess_idx = word_to_index[guess_word]

    # –Ø–∫—â–æ –≤–≥–∞–¥–∞–ª–∏
    if secret_idx == guess_idx:
        return GuessResponse(position=1, normalized_word=words_list[guess_idx], found=True)

    # –†–∞—Ö—É—î–º–æ –ø–æ–∑–∏—Ü—ñ—é
    pos = get_position(secret_idx, guess_idx)
    
    return GuessResponse(
        position=pos,
        normalized_word=words_list[guess_idx],
        found=True,
    )

@app.post("/api/giveup")
def api_giveup(req: GuessRequest):
    """–Ø–∫—â–æ –≥—Ä–∞–≤–µ—Ü—å –∑–¥–∞—î—Ç—å—Å—è"""
    if not req.seed_date:
        req.seed_date = date.today().isoformat()
    secret_idx = get_secret_index(req.level, req.seed_date)
    return {"secret_word": words_list[secret_idx]}

# –ü—ñ–¥–∫–ª—é—á–∞—î–º–æ –ø–∞–ø–∫—É frontend —è–∫ —Å—Ç–∞—Ç–∏—á–Ω–∏–π —Å–∞–π—Ç
FRONTEND_PATH = BASE / "frontend"
if FRONTEND_PATH.exists():
    app.mount("/", StaticFiles(directory=str(FRONTEND_PATH), html=True), name="frontend")

if __name__ == "__main__":
    import uvicorn
    print("üöÄ –ó–∞–ø—É—Å–∫–∞—î–º–æ —Å–µ—Ä–≤–µ—Ä Contexto UA...")
    uvicorn.run(app, host="0.0.0.0", port=8000)